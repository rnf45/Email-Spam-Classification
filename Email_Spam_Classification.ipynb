{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rnf45/Email-Spam-Classification/blob/main/Email_Spam_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "1. load csv file\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uxgBX0YXu1du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import NumPy library for numerical operations\n",
        "import numpy as np\n",
        "\n",
        "# import Pandas library for data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# import train-test split and K-Fold cross-validation from scikit-learn\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "def load_data(file_path):\n",
        "\n",
        "    # read CSV file using Pandas\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # extract feature matrix (excluding the last column)\n",
        "    X = data.iloc[:, :-1].values\n",
        "\n",
        "    # extract target variable (last column)\n",
        "    y = data.iloc[:, -1].values\n",
        "\n",
        "    # return feature matrix and target variable\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "f8zPzgkbub4C"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. split dataset"
      ],
      "metadata": {
        "id": "ld-jKb6Qu6vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(X, y, test_size=0.2):\n",
        "\n",
        "    # split the data using train_test_split from scikit-learn\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "    # return the training and test sets\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "CAwuPoMyu_PY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive bayes\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "FXyRfd35yRPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes algorithm\n",
        "def naive_bayes(X_train, y_train, X_test):\n",
        "\n",
        "    # remove last four attributes from training set\n",
        "    X_train = X_train[:, :-4]\n",
        "\n",
        "    # remove last four attributes from test set\n",
        "    X_test = X_test[:, :-4]\n",
        "\n",
        "    # calculate prior probabilities of each class\n",
        "    class_prob = np.bincount(y_train) / len(y_train)\n",
        "\n",
        "    # initialize attribute probabilities matrix\n",
        "    attr_prob = np.zeros((2, X_train.shape[1]))\n",
        "\n",
        "    # iterate over classes (0 and 1)\n",
        "    for c in range(2):\n",
        "\n",
        "        # select instances belonging to class c\n",
        "        X_c = X_train[y_train == c]\n",
        "\n",
        "        # calculate attribute probabilities using Laplace smoothing\n",
        "        attr_prob[c] = (X_c.sum(axis=0) + 1) / (X_c.shape[0] + 2)\n",
        "\n",
        "    # initialize empty list to store predictions\n",
        "    predictions = []\n",
        "\n",
        "    # iterate over each instance in test set\n",
        "    for x in X_test:\n",
        "\n",
        "        # calculate log probabilities for each class\n",
        "        probs = [np.log(class_prob[c]) + np.sum(np.log(attr_prob[c, x > 0])) for c in range(2)]\n",
        "\n",
        "        # append predicted class with highest probability to predictions list\n",
        "        predictions.append(np.argmax(probs))\n",
        "\n",
        "    # return predictions as a NumPy array\n",
        "    return np.array(predictions)"
      ],
      "metadata": {
        "id": "-caCmchovWiO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#KNN - k-Nearest Neighbor"
      ],
      "metadata": {
        "id": "1A1HT8gcxXLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cosine similarity function\n",
        "def cosine_similarity(x1, x2):\n",
        "\n",
        "    # calculate cosine similarity between x1 and x2 using numpy.cos()\n",
        "    cos_sim = np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))\n",
        "\n",
        "    return np.cos(np.arccos(cos_sim))\n",
        "\n",
        "\n",
        "\n",
        "# K-Nearest Neighbor\n",
        "def knn(X_train, y_train, X_test, k=5):\n",
        "\n",
        "    # initialize empty list to store predictions\n",
        "    predictions = []\n",
        "\n",
        "    # iterate over each test instance\n",
        "    for x in X_test:\n",
        "\n",
        "        # calculate cosine similarities between x and each instance in X_train\n",
        "        similarities = [cosine_similarity(x, x_train) for x_train in X_train]\n",
        "\n",
        "        # find indices of k nearest neighbors\n",
        "        nearest_indices = np.argsort(similarities)[-k:]\n",
        "\n",
        "        # retrieve labels of k nearest neighbors\n",
        "        nearest_labels = y_train[nearest_indices]\n",
        "\n",
        "        # predict class based on majority vote of k nearest neighbors\n",
        "        prediction = np.bincount(nearest_labels).argmax()\n",
        "\n",
        "        # append predicted class to predictions list\n",
        "        predictions.append(prediction)\n",
        "\n",
        "    # return predictions as a NumPy array\n",
        "    return np.array(predictions)"
      ],
      "metadata": {
        "id": "C5iKBNl1xPRe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LR - Logistic Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "OUzUupva0Fxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sigmoid activation function\n",
        "def sigmoid(z):\n",
        "\n",
        "    # calculate the sigmoid of z\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Logistic Regression algorithm\n",
        "def logistic_regression(X_train, y_train, X_test, epochs=100, learning_rate=0.01):\n",
        "\n",
        "    # add column of ones to training set for bias term\n",
        "    X_train = np.c_[np.ones((len(X_train), 1)), X_train]\n",
        "\n",
        "    # add column of ones to test set for bias term\n",
        "    X_test = np.c_[np.ones((len(X_test), 1)), X_test]\n",
        "\n",
        "    # initialize weight matrix with random values\n",
        "    M = np.random.randn(X_train.shape[1], 1)\n",
        "\n",
        "    # train logistic regression model for specified number of epochs\n",
        "    for _ in range(epochs):\n",
        "\n",
        "        # calculate predicted probabilities using sigmoid function\n",
        "        pred_y = sigmoid(np.dot(X_train, M))\n",
        "\n",
        "        # calculate cross-entropy loss\n",
        "        loss = -np.mean(y_train * np.log(pred_y) + (1 - y_train) * np.log(1 - pred_y))\n",
        "\n",
        "        # calculate gradient of loss with respect to weights\n",
        "        gm = np.dot(X_train.T, (pred_y - y_train.reshape(-1, 1))) * 2 / len(X_train)\n",
        "\n",
        "        # update weights using gradient descent\n",
        "        M -= learning_rate * gm\n",
        "\n",
        "    # make predictions on test set using trained weights\n",
        "    predictions = (sigmoid(np.dot(X_test, M)) >= 0.5).astype(int).flatten()\n",
        "\n",
        "    # return predictions\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "ouAosryOyJPH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n"
      ],
      "metadata": {
        "id": "mAssSW_I0GvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(y_true, y_pred):\n",
        "\n",
        "    # calculate accuracy of model\n",
        "    accuracy = np.mean(y_true == y_pred)\n",
        "\n",
        "    # calculate number of false positives\n",
        "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
        "\n",
        "    # calculate number of true positives\n",
        "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
        "\n",
        "    # calculate number of false negatives\n",
        "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
        "\n",
        "    # calculate number of true negatives\n",
        "    tn = np.sum((y_pred == 0) & (y_true == 0))\n",
        "\n",
        "    # calculate false positive rate\n",
        "    fpr = fp / (fp + tn)\n",
        "\n",
        "    # calculate true positive rate\n",
        "    tpr = tp / (tp + fn)\n",
        "\n",
        "    # calculate area under ROC curve\n",
        "    auc = 0.5 * (tpr + 1 - fpr)\n",
        "\n",
        "    # return evaluation metrics\n",
        "    return accuracy, fpr, tpr, auc"
      ],
      "metadata": {
        "id": "e0MQ0eo0MnmB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    # load the dataset\n",
        "    X, y = load_data(\"spambase.csv\")\n",
        "\n",
        "    # split dataset into training and test sets\n",
        "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
        "\n",
        "    # perform 5-fold cross-validation on training set\n",
        "    kf = KFold(n_splits=5)\n",
        "\n",
        "    # iterate over each fold\n",
        "    for train_index, val_index in kf.split(X_train):\n",
        "\n",
        "        # split training set into training and validation subsets for current fold\n",
        "        X_sub_train, X_sub_val = X_train[train_index], X_train[val_index]\n",
        "        y_sub_train, y_sub_val = y_train[train_index], y_train[val_index]\n",
        "\n",
        "        # train and evaluate Naive Bayes model\n",
        "        nb_predictions = naive_bayes(X_sub_train, y_sub_train, X_sub_val)\n",
        "        nb_accuracy, nb_fpr, nb_tpr, nb_auc = evaluate_model(y_sub_val, nb_predictions)\n",
        "        print(\"Naive Bayes - Accuracy: {:.2f}, FPR: {:.2f}, TPR: {:.2f}, AUC: {:.2f}\".format(\n",
        "            nb_accuracy, nb_fpr, nb_tpr, nb_auc))\n",
        "\n",
        "        # train and evaluate KNN model\n",
        "        knn_predictions = knn(X_sub_train, y_sub_train, X_sub_val)\n",
        "        knn_accuracy, knn_fpr, knn_tpr, knn_auc = evaluate_model(y_sub_val, knn_predictions)\n",
        "        print(\"KNN - Accuracy: {:.2f}, FPR: {:.2f}, TPR: {:.2f}, AUC: {:.2f}\".format(\n",
        "            knn_accuracy, knn_fpr, knn_tpr, knn_auc))\n",
        "\n",
        "        # train and evaluate Logistic Regression model\n",
        "        lr_predictions = logistic_regression(X_sub_train, y_sub_train, X_sub_val)\n",
        "        lr_accuracy, lr_fpr, lr_tpr, lr_auc = evaluate_model(y_sub_val, lr_predictions)\n",
        "        print(\"Logistic Regression - Accuracy: {:.2f}, FPR: {:.2f}, TPR: {:.2f}, AUC: {:.2f}\".format(\n",
        "            lr_accuracy, lr_fpr, lr_tpr, lr_auc))\n",
        "\n",
        "    # evaluate Naive Bayes model on test set\n",
        "    nb_test_predictions = naive_bayes(X_train, y_train, X_test)\n",
        "    nb_test_accuracy, nb_test_fpr, nb_test_tpr, nb_test_auc = evaluate_model(y_test, nb_test_predictions)\n",
        "    print(\"Naive Bayes (Test Set) - Accuracy: {:.2f}, FPR: {:.2f}, TPR: {:.2f}, AUC: {:.2f}\".format(\n",
        "        nb_test_accuracy, nb_test_fpr, nb_test_tpr, nb_test_auc))\n",
        "\n",
        "    # evaluate KNN model on test set\n",
        "    knn_test_predictions = knn(X_train, y_train, X_test)\n",
        "    knn_test_accuracy, knn_test_fpr, knn_test_tpr, knn_test_auc = evaluate_model(y_test, knn_test_predictions)\n",
        "    print(\"KNN (Test Set) - Accuracy: {:.2f}, FPR: {:.2f}, TPR: {:.2f}, AUC: {:.2f}\".format(\n",
        "        knn_test_accuracy, knn_test_fpr, knn_test_tpr, knn_test_auc))\n",
        "\n",
        "    # evaluate Logistic Regression model on test set\n",
        "    lr_test_predictions = logistic_regression(X_train, y_train, X_test)\n",
        "    lr_test_accuracy, lr_test_fpr, lr_test_tpr, lr_test_auc = evaluate_model(y_test, lr_test_predictions)\n",
        "    print(\"Logistic Regression (Test Set) - Accuracy: {:.2f}, FPR: {:.2f}, TPR: {:.2f}, AUC: {:.2f}\".format(\n",
        "        lr_test_accuracy, lr_test_fpr, lr_test_tpr, lr_test_auc))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Calling the main function\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNeQuENHzlA8",
        "outputId": "3a6fb92b-e525-420d-e5a2-efef11b6d8d9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes - Accuracy: 0.86, FPR: 0.21, TPR: 0.97, AUC: 0.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-348ad4c6e190>:7: RuntimeWarning: invalid value encountered in arccos\n",
            "  return np.cos(np.arccos(cos_sim))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN - Accuracy: 0.84, FPR: 0.18, TPR: 0.87, AUC: 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-438e6610858c>:26: RuntimeWarning: divide by zero encountered in log\n",
            "  loss = -np.mean(y_train * np.log(pred_y) + (1 - y_train) * np.log(1 - pred_y))\n",
            "<ipython-input-12-438e6610858c>:26: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.mean(y_train * np.log(pred_y) + (1 - y_train) * np.log(1 - pred_y))\n",
            "<ipython-input-12-438e6610858c>:5: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression - Accuracy: 0.62, FPR: 0.02, TPR: 0.03, AUC: 0.51\n",
            "Naive Bayes - Accuracy: 0.85, FPR: 0.22, TPR: 0.96, AUC: 0.87\n",
            "KNN - Accuracy: 0.80, FPR: 0.23, TPR: 0.86, AUC: 0.82\n",
            "Logistic Regression - Accuracy: 0.43, FPR: 0.88, TPR: 0.95, AUC: 0.53\n",
            "Naive Bayes - Accuracy: 0.89, FPR: 0.16, TPR: 0.96, AUC: 0.90\n",
            "KNN - Accuracy: 0.84, FPR: 0.18, TPR: 0.86, AUC: 0.84\n",
            "Logistic Regression - Accuracy: 0.46, FPR: 0.93, TPR: 1.00, AUC: 0.53\n",
            "Naive Bayes - Accuracy: 0.87, FPR: 0.18, TPR: 0.97, AUC: 0.89\n",
            "KNN - Accuracy: 0.82, FPR: 0.19, TPR: 0.85, AUC: 0.83\n",
            "Logistic Regression - Accuracy: 0.45, FPR: 0.86, TPR: 0.96, AUC: 0.55\n",
            "Naive Bayes - Accuracy: 0.88, FPR: 0.16, TPR: 0.94, AUC: 0.89\n",
            "KNN - Accuracy: 0.86, FPR: 0.15, TPR: 0.89, AUC: 0.87\n",
            "Logistic Regression - Accuracy: 0.60, FPR: 0.60, TPR: 0.92, AUC: 0.66\n",
            "Naive Bayes (Test Set) - Accuracy: 0.89, FPR: 0.18, TPR: 0.98, AUC: 0.90\n",
            "KNN (Test Set) - Accuracy: 0.85, FPR: 0.18, TPR: 0.88, AUC: 0.85\n",
            "Logistic Regression (Test Set) - Accuracy: 0.51, FPR: 0.85, TPR: 0.99, AUC: 0.57\n"
          ]
        }
      ]
    }
  ]
}